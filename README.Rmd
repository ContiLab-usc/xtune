---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```


xtune: Tuning feature-specific shrinkage parameters of penalized regression models based on external information 

<!-- badges: start -->

<!-- badges: end -->

=======

&#x1F4D7;  Introduction
-----------------------

### Motivation

In standard regularized regression (Lasso, Ridge, and Elastic-net), a single penalty parameter $\lambda$ applied equally to all regression coefficients to control the amount of regularization in the model.

Better prediction accuracy may be achieved by allowing a **different amount of shrinkage**. Ideally, we want to give a small penalty to important features and a large penalty to unimportant features. We guide the penalized regression model with external data $**Z**$ that are potentially informative for the importance/effect size of coefficients and allow feature-specific shrinkage modeled as a log-linear function of the external data. 

The objective function of feature-specific shrinkage integrating external information is: 

$$\min_f \sum_{i = 1}^n &V(f(x_i), y_i) + \textcolor{red}{\bm\lambda} R(f)$$
        $$\textcolor{red}{\bm\lambda = e^{\bm Z \cdot \bm \alpha}}$$

where $V$ represents loss function, $\lambda$ is the penalty/tuning parameter, and $R(f)$ is the  regularization/penalty term. Specifically, we use Elastic-net type of penalty:

$$R(f) = \left[\sum_{k = 1}^K\bigg((1-c)||\beta_k||_2^2/2 + c||\beta_k||_1 \bigg) \right]$$

when $c = 1, 0$ or any value between 0 to 1, the model is equivalent to LASSO, Ridge, and Elastic-net, respectively.

The idea of external data is that it provides us information on the importance/effect size of regression coefficients. It could be any nominal or quantitative feature-specific information, such as the grouping of predictors, prior knowledge of biological importance, external p-values, function annotations, etc. Each column of Z is a variable for features in design matrix X. Z is of dimension $p \times q$, where $p$ is the number of features and $q$ is the number of variables in $Z$.

### Tuning multiple penalty parameters

Penalized regression fitting consists of two phases: (1) learning the tuning parameter(s) (2) estimating the regression coefficients giving the tuning parameter(s). Phase (1) is the key to achieve good performance. Cross-validation is widely used to tune a single penalty parameter, but it is computationally infeasible to tune more than three penalty parameters. We propose an **Empirical Bayes** approach to estimate the multiple tuning parameters. The individual penalties are interpreted as variance terms of the priors (exponential prior for Elastic-net) in a random effect formulation of penalized regressions. A majorization-minimization algorithm is employed for implementation. Once the tuning parameters $\lambda$s are estimated, and therefore the penalties are known, phase (2) - estimating the regression coefficients is done using `glmnet`. 

### Data structure examples

Suppose we want to predict a person's weight loss using his/her weekly dietary intake. Our external information Z could incorporate information about the levels of relevant food constituents in the dietary items.

Primary data X and Y: predicting an individual's weight loss by his/her weekly dietary items intake 


External information Z: the nutrition facts about each dietary item


&#x1F4D9;  Installation
-----------------------
`xtune` can be installed from Github using the following command:

``` r
# install.packages("devtools")

library(devtools)
devtools::install_github("JingxuanH/xtune", 
                         build_vignettes = TRUE)

library(xtune)
```

&#x270D; Citation
-----------------------

* **`xtune` LASSO**: Zeng, Chubing, Duncan Campbell Thomas, and Juan Pablo Lewinger. "Incorporating prior knowledge into regularized regression." Bioinformatics 37.4 (2021): 514-521.

* **`xtune` classification with Elastic-net type of penalty**: paper coming soon

* **`xtune`** package:
```{r}
citation("xtune")
```

Feel free to contact `hejingxu@usc.edu` if you have any questions.


&#x1F4D8;  Examples
-------------------
To show some examples on how to use this package, we simulated an example of data that contains 100 observations, 200 predictors, and a continuous outcome. The external information Z contains 4 columns, each column is indicator variable (can be viewed as the grouping of predictors). 


```{r example}
library(xtune)

## load the example data
data(example)
```

The data looks like: 
```{r}
example$X[1:3,1:5]
example$Z[1:5,]
```

`xtune()` is the core function to fit the integrated penalized regression model. At a minimum, you need to specify the predictor matrix `X`, outcome variable `Y`. If an external information matrix `Z` is provided, the function will incorporate `Z` to allow differential shrinkage based on Z. The estimated tuning parameters are returned in `$penalty.vector`. 

If you do not provide external information `Z`, the function will perform empirical Bayes tuning to choose the single penalty parameter in penalized regression, as an alternative to cross-validation. You could compare the tuning parameter chosen by empirical Bayes tuning to that choose by cross-validation (see also `cv.glmnet`). The default penalty applied to the predictors is the Elastic-net penalty. 

If you provide an identify matrix as external information Z to `xtune()`, the function will estimate a separate tuning parameter $\lambda_j$ for each regression coefficient $\beta_j$.

```{r, message=FALSE}
xtune.fit <- xtune(example$X,example$Y,example$Z, family = "linear")
```

To view the penalty parameters estimated by `xtune()`

```{r}
xtune.fit$penalty.vector[1:5]
```

The `coef` and `predict` functions can be used to extract beta coefficient estimates and predict response on new data. 

```{r}
coef_xtune(xtune.fit)[1:5]
predict_xtune(xtune.fit, example$X)[1:5]
```

More details and examples are also described in the vignettes to further illustrate the usage and syntax of this package. 

